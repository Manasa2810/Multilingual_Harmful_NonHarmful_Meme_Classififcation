{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPfr26/SQdE4IxlApRJ2H7i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akhilesh348/Multilingual-Memes-Classification-Harmful-Non-Harmful-/blob/main/Text_Encoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytesseract\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "pW7-hD-8Ho6Y"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "zip_files = [\n",
        "    \"/content/Harmful Telugu memes.zip\",\n",
        "    \"/content/Non-Harmful Telugu memes.zip\"\n",
        "]\n",
        "\n",
        "for zip_path in zip_files:\n",
        "    folder_name = os.path.splitext(os.path.basename(zip_path))[0]\n",
        "    extract_to = f\"/content/{folder_name}\"\n",
        "    os.makedirs(extract_to, exist_ok=True)\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "    print(f\" Extracted {zip_path} â†’ {extract_to}\")"
      ],
      "metadata": {
        "id": "FJft4kD-J0TC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "from pathlib import Path\n",
        "from typing import Generator, Optional, Tuple\n",
        "import csv\n",
        "\n",
        "try:\n",
        "    from PIL import Image\n",
        "except ImportError:\n",
        "    print(\"Pillow is required. Install with: pip install pillow\")\n",
        "    raise\n",
        "\n",
        "try:\n",
        "    import pytesseract\n",
        "except ImportError:\n",
        "    print(\"pytesseract is required. Install with: pip install pytesseract\")\n",
        "    raise\n",
        "\n",
        "try:\n",
        "    import cv2\n",
        "    import numpy as np\n",
        "    HAS_OPENCV = True\n",
        "    HAS_CUDA = cv2.cuda.getCudaEnabledDeviceCount() > 0\n",
        "except ImportError:\n",
        "    HAS_OPENCV = False\n",
        "    HAS_CUDA = False\n",
        "    print(\"Warning: opencv-python not found. Preprocessing will be skipped.\")\n",
        "\n",
        "IMAGE_EXTENSIONS = {'.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif', '.webp'}\n",
        "\n",
        "# Define dataset paths directly\n",
        "SRC_FOLDERS = [\n",
        "    Path(\"/content/Harmful Telugu memes\"),\n",
        "    Path(\"/content/Non-Harmful Telugu memes\"),\n",
        "]\n",
        "OUT_DIR = Path(\"/content/output\")\n",
        "LANG = \"tel+eng\"\n",
        "USE_PREPROCESS = True\n",
        "WORKERS = 2\n",
        "\n",
        "\n",
        "def preprocess_image_for_ocr(image_path: Path):\n",
        "    \"\"\"GPU-accelerated preprocessing using OpenCV CUDA if available.\"\"\"\n",
        "    if not HAS_OPENCV:\n",
        "        raise RuntimeError(\"opencv-python required for preprocessing.\")\n",
        "\n",
        "    img = cv2.imread(str(image_path))\n",
        "    if img is None:\n",
        "        raise ValueError(f\"Failed to read image: {image_path}\")\n",
        "\n",
        "    if HAS_CUDA:\n",
        "        try:\n",
        "            gpu_img = cv2.cuda_GpuMat()\n",
        "            gpu_img.upload(img)\n",
        "            gpu_gray = cv2.cuda.cvtColor(gpu_img, cv2.COLOR_BGR2GRAY)\n",
        "            gpu_blur = cv2.cuda.bilateralFilter(gpu_gray, 9, 75, 75)\n",
        "            gray = gpu_blur.download()\n",
        "        except Exception as e:\n",
        "            print(f\" CUDA preprocessing failed for {image_path.name}, using CPU: {e}\")\n",
        "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    else:\n",
        "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        gray = cv2.bilateralFilter(gray, 9, 75, 75)\n",
        "\n",
        "    th = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "                               cv2.THRESH_BINARY, 11, 2)\n",
        "    return th\n",
        "\n",
        "\n",
        "def extract_text_from_image(image_path: Path, use_preprocess: bool, lang: str = 'eng') -> str:\n",
        "    \"\"\"Perform OCR using pytesseract.\"\"\"\n",
        "    try:\n",
        "        if use_preprocess:\n",
        "            img = preprocess_image_for_ocr(image_path)\n",
        "            text = pytesseract.image_to_string(img, lang=lang)\n",
        "        else:\n",
        "            with Image.open(image_path) as pil_img:\n",
        "                text = pytesseract.image_to_string(pil_img, lang=lang)\n",
        "        return text.strip()\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"OCR failed for {image_path.name}: {e}\") from e\n",
        "\n",
        "\n",
        "def find_images(folder: Path) -> Generator[Path, None, None]:\n",
        "    \"\"\"Find all image files in a folder recursively.\"\"\"\n",
        "    for p in sorted(folder.rglob('*')):\n",
        "        if p.is_file() and p.suffix.lower() in IMAGE_EXTENSIONS:\n",
        "            yield p\n",
        "\n",
        "\n",
        "def process_single_image(args_tuple: Tuple[Path, bool, str]) -> Tuple[str, str, Optional[str]]:\n",
        "    \"\"\"OCR one image.\"\"\"\n",
        "    img_path, use_preprocess, lang = args_tuple\n",
        "    try:\n",
        "        text = extract_text_from_image(img_path, use_preprocess, lang)\n",
        "        return (img_path.name, text, None)\n",
        "    except Exception as e:\n",
        "        return (img_path.name, \"\", str(e))\n",
        "\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        pytesseract.get_tesseract_version()\n",
        "    except Exception as e:\n",
        "        print(\"Error: Tesseract not found. Please install it.\")\n",
        "        print(f\"Details: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    images = []\n",
        "    for folder in SRC_FOLDERS:\n",
        "        if not folder.exists():\n",
        "            print(f\" Warning: Folder not found - {folder}\")\n",
        "            continue\n",
        "        folder_images = list(find_images(folder))\n",
        "        images.extend(folder_images)\n",
        "        images=images\n",
        "        print(f\"Found {len(folder_images)} images in {folder.name}\")\n",
        "\n",
        "    if not images:\n",
        "        print(\" No images found in provided folders.\")\n",
        "        sys.exit(0)\n",
        "\n",
        "    print(f\"\\n Total images: {len(images)}\")\n",
        "    print(f\" Language: {LANG}, Preprocess: {USE_PREPROCESS}\")\n",
        "    print(f\" Using {'GPU' if HAS_CUDA else 'CPU'} for preprocessing.\\n\")\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Parallel OCR\n",
        "    if WORKERS > 1:\n",
        "        tasks = [(img, USE_PREPROCESS, LANG) for img in images]\n",
        "        with ProcessPoolExecutor(max_workers=WORKERS) as executor:\n",
        "            futures = {executor.submit(process_single_image, t): t[0] for t in tasks}\n",
        "            for future in as_completed(futures):\n",
        "                name, text, error = future.result()\n",
        "                if error:\n",
        "                    print(f\" Error: {name} - {error}\")\n",
        "                else:\n",
        "                    print(f\" OCR done: {name}\")\n",
        "                results.append((name, text))\n",
        "    else:\n",
        "        for img in images:\n",
        "            name, text, error = process_single_image((img, USE_PREPROCESS, LANG))\n",
        "            if error:\n",
        "                print(f\" Error: {name} - {error}\")\n",
        "            else:\n",
        "                print(f\" OCR done: {name}\")\n",
        "            results.append((name, text))\n",
        "\n",
        "    # Save combined CSV\n",
        "    csv_path = OUT_DIR / \"test.csv\"\n",
        "    with open(csv_path, \"w\", newline='', encoding=\"utf-8\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"imageid\", \"text\"])\n",
        "        for idx, (name, text) in enumerate(results, start=1):\n",
        "            image_id = f\"img{idx}\"\n",
        "            writer.writerow([image_id, text.replace(\"\\n\", \" \").strip()])\n",
        "\n",
        "    print(f\"\\n Combined CSV file saved at: {csv_path}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "I3ztaFkeImwx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from tqdm import tqdm\n",
        "\n",
        "#Load model and tokenizer\n",
        "model_name = \"ai4bharat/indic-bert\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "model.eval()  # Set model to evaluation mode\n",
        "\n",
        "# Load CSV file\n",
        "df = pd.read_csv(\"/content/test.csv\")\n",
        "print(\"Loaded rows:\", len(df))\n",
        "print(df.head())\n",
        "\n",
        "# Encode Telugu text\n",
        "embeddings_list = []\n",
        "\n",
        "for text in tqdm(df['text'], desc=\"Encoding texts\"):\n",
        "    try:\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        # Mean pooling\n",
        "        emb = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
        "        embeddings_list.append(emb)\n",
        "    except Exception as e:\n",
        "        print(f\"Error encoding: {text}\\n{e}\")\n",
        "        embeddings_list.append([0]*768)  # fallback vector\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "embeddings_array = np.vstack(embeddings_list)\n",
        "print(\"Final embeddings shape:\", embeddings_array.shape)\n",
        "\n",
        "#Save as NumPy file\n",
        "np.save(\"text_embeddings.npy\", embeddings_array)\n",
        "\n",
        "#Save as CSV (with image_id for reference)\n",
        "emb_df = pd.DataFrame(embeddings_array)\n",
        "emb_df.insert(0, \"imageid\", df['imageid'])\n",
        "emb_df.to_csv(\"text_embeddings.csv\", index=False)\n",
        "\n",
        "print(\" Saved embeddings to text_embeddings.csv and text_embeddings.npy\")\n"
      ],
      "metadata": {
        "id": "GdX8pmhsHxe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6E1N9DRBJ2u1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}