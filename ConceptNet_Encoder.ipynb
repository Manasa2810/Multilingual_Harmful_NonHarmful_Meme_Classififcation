{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1PhFTu3YGaTzLio4MnD4CtpjnbBnlW3gU",
      "authorship_tag": "ABX9TyMruR+OzUAr3hyvU+lCv7JW"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Tuple\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from time import sleep\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
        "import spacy\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "IMAGE_FOLDER = \"/content/drive/MyDrive/test image\"\n",
        "OUTPUT_DIR = \"conceptnet_encodings\"\n",
        "TOP_N_CONCEPTNET = 10\n",
        "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "CAPTION_MODEL_NAME = \"nlpconnect/vit-gpt2-image-captioning\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCH_SIZE = 8\n",
        "CONCEPTNET_CACHE_FILE = \"conceptnet_cache.json\"\n",
        "REQUESTS_SLEEP = 0.1\n",
        "\n",
        "print(\"Loading captioning model...\")\n",
        "caption_model = VisionEncoderDecoderModel.from_pretrained(CAPTION_MODEL_NAME).to(DEVICE)\n",
        "caption_processor = ViTImageProcessor.from_pretrained(CAPTION_MODEL_NAME)\n",
        "caption_tokenizer = AutoTokenizer.from_pretrained(CAPTION_MODEL_NAME)\n",
        "\n",
        "if caption_tokenizer.pad_token is None:\n",
        "    caption_tokenizer.pad_token = caption_tokenizer.eos_token\n",
        "    caption_tokenizer.pad_token_id = caption_tokenizer.eos_token_id\n",
        "\n",
        "try:\n",
        "    caption_model.config.pad_token_id = caption_tokenizer.pad_token_id\n",
        "    caption_model.config.decoder_start_token_id = getattr(caption_tokenizer, \"cls_token_id\", None) or caption_tokenizer.eos_token_id\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "GEN_KWARGS = {\n",
        "    \"max_length\": 30,\n",
        "    \"num_beams\": 4,\n",
        "    \"early_stopping\": True,\n",
        "    \"no_repeat_ngram_size\": 2,\n",
        "}\n",
        "\n",
        "def generate_caption(image: Image.Image) -> str:\n",
        "    \"\"\"Generate a caption for a single PIL Image.\"\"\"\n",
        "    if image.mode != \"RGB\":\n",
        "        image = image.convert(\"RGB\")\n",
        "    pixel_values = caption_processor(images=image, return_tensors=\"pt\").pixel_values.to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        output_ids = caption_model.generate(pixel_values, **GEN_KWARGS)\n",
        "    caption = caption_tokenizer.decode(output_ids[0], skip_special_tokens=True).strip()\n",
        "    return caption\n",
        "\n",
        "print(\"Loading spaCy model...\")\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def extract_concept_words(caption: str) -> List[str]:\n",
        "    \"\"\"Extract nouns & noun-chunks from caption text.\"\"\"\n",
        "    doc = nlp(caption)\n",
        "    concepts = []\n",
        "    for nc in doc.noun_chunks:\n",
        "        t = nc.text.strip().lower()\n",
        "        if len(t) > 1:\n",
        "            concepts.append(t)\n",
        "    for tok in doc:\n",
        "        if tok.pos_ in {\"NOUN\", \"PROPN\"}:\n",
        "            t = tok.lemma_.lower().strip()\n",
        "            if len(t) > 1 and t not in {\"image\", \"photo\", \"picture\"}:\n",
        "                concepts.append(t)\n",
        "    seen = set()\n",
        "    ordered = []\n",
        "    for c in concepts:\n",
        "        if c not in seen:\n",
        "            seen.add(c)\n",
        "            ordered.append(c)\n",
        "    return ordered\n",
        "\n",
        "CONCEPTNET_BASE = \"https://api.conceptnet.io\"\n",
        "\n",
        "def load_conceptnet_cache(path: str) -> dict:\n",
        "    if os.path.exists(path):\n",
        "        try:\n",
        "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "                return json.load(f)\n",
        "        except Exception:\n",
        "            return {}\n",
        "    return {}\n",
        "\n",
        "def save_conceptnet_cache(path: str, cache: dict):\n",
        "    tmp = path + \".tmp\"\n",
        "    with open(tmp, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(cache, f, ensure_ascii=False, indent=2)\n",
        "    os.replace(tmp, path)\n",
        "\n",
        "cn_cache = load_conceptnet_cache(CONCEPTNET_CACHE_FILE)\n",
        "\n",
        "def conceptnet_related(concept: str, topn: int = 10) -> List[Tuple[str, float]]:\n",
        "    \"\"\"\n",
        "    Query ConceptNet for related concepts, with caching.\n",
        "    Returns list of (surface_text, weight) sorted by weight desc.\n",
        "    \"\"\"\n",
        "    key = concept.strip().lower()\n",
        "    if key in cn_cache:\n",
        "        return cn_cache[key][:topn]\n",
        "\n",
        "    concept_clean = key.replace(\" \", \"_\")\n",
        "    url = f\"{CONCEPTNET_BASE}/related/c/en/{concept_clean}\"\n",
        "    params = {\"filter\": \"/c/en\"}\n",
        "    try:\n",
        "        resp = requests.get(url, params=params, timeout=8)\n",
        "        resp.raise_for_status()\n",
        "        j = resp.json()\n",
        "        related = []\n",
        "        for item in j.get(\"related\", [])[:topn]:\n",
        "            surface = item.get(\"surfaceText\") or item.get(\"@id\", \"\").split(\"/\")[-1]\n",
        "            if isinstance(surface, str):\n",
        "                surface = surface.replace(\"_\", \" \").lower()\n",
        "                related.append((surface, float(item.get(\"weight\", 0.0))))\n",
        "        related.sort(key=lambda x: -x[1])\n",
        "    except Exception:\n",
        "        related = []\n",
        "    cn_cache[key] = related\n",
        "    sleep(REQUESTS_SLEEP)\n",
        "    return related[:topn]\n",
        "\n",
        "print(\"Loading embedding model...\")\n",
        "embed_model = SentenceTransformer(EMBEDDING_MODEL_NAME, device=DEVICE)\n",
        "\n",
        "def build_concept_bag(concepts: List[str], cn_expand_topn: int = 8) -> str:\n",
        "    \"\"\"\n",
        "    Given list of concept words, expand each with ConceptNet and return a single combined text.\n",
        "    Example output: \"dog; pet; canine; animal; fur; leash\"\n",
        "    \"\"\"\n",
        "    bag = []\n",
        "    for c in concepts:\n",
        "        bag.append(c)\n",
        "        related = conceptnet_related(c, topn=cn_expand_topn)\n",
        "        for (term, weight) in related:\n",
        "            bag.append(term)\n",
        "    seen = set()\n",
        "    ordered = []\n",
        "    for t in bag:\n",
        "        if t not in seen and isinstance(t, str) and len(t) > 0:\n",
        "            seen.add(t)\n",
        "            ordered.append(t)\n",
        "    return \"; \".join(ordered)\n",
        "\n",
        "def embed_texts(texts: List[str]) -> np.ndarray:\n",
        "    \"\"\"Batch encode texts using sentence-transformers; returns numpy array.\"\"\"\n",
        "    embeddings = embed_model.encode(texts, batch_size=BATCH_SIZE, convert_to_numpy=True, show_progress_bar=False)\n",
        "    return embeddings\n",
        "\n",
        "def find_images(folder: str, exts={\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\"}) -> List[Path]:\n",
        "    p = Path(folder)\n",
        "    files = [f for f in p.rglob(\"*\") if f.suffix.lower() in exts]\n",
        "    files.sort()\n",
        "    return files\n",
        "\n",
        "def process_images(image_folder: str, output_dir: str):\n",
        "    outp = Path(output_dir)\n",
        "    outp.mkdir(parents=True, exist_ok=True)\n",
        "    images = find_images(image_folder)\n",
        "    print(f\"Found {len(images)} images in {image_folder}\")\n",
        "\n",
        "    records = []\n",
        "    concept_bag_texts = []\n",
        "    skipped = 0\n",
        "\n",
        "    for img_path in tqdm(images, desc=\"Processing images\"):\n",
        "        try:\n",
        "            img = Image.open(img_path)\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping {img_path}: cannot open ({e})\")\n",
        "            skipped += 1\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            caption = generate_caption(img)\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: failed to caption {img_path}: {e}\")\n",
        "            caption = \"\"\n",
        "\n",
        "        concepts = extract_concept_words(caption) if caption else []\n",
        "        concept_bag = build_concept_bag(concepts, cn_expand_topn=TOP_N_CONCEPTNET) if concepts else \"\"\n",
        "\n",
        "        records.append({\n",
        "            \"filename\": str(img_path),\n",
        "            \"caption\": caption,\n",
        "            \"extracted_concepts\": \"|\".join(concepts),\n",
        "            \"concept_bag\": concept_bag\n",
        "        })\n",
        "        concept_bag_texts.append(concept_bag if concept_bag else \"\")\n",
        "\n",
        "    print(\"Embedding concept bags...\")\n",
        "    if len(concept_bag_texts) == 0:\n",
        "        print(\"No concept bags to embed.\")\n",
        "        return\n",
        "\n",
        "    embeddings = embed_texts(concept_bag_texts)\n",
        "    np.save(outp / \"image_concept_embeddings.npy\", embeddings)\n",
        "    df = pd.DataFrame(records)\n",
        "    df.to_csv(outp / \"image_concept_metadata.csv\", index=False)\n",
        "\n",
        "    save_conceptnet_cache(CONCEPTNET_CACHE_FILE, cn_cache)\n",
        "\n",
        "    print(f\"Saved embeddings ({embeddings.shape}) and metadata to {outp}. Skipped {skipped} files.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    process_images(IMAGE_FOLDER, OUTPUT_DIR)"
      ],
      "metadata": {
        "id": "0UCPLodwU458"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}